{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'native-country': 'United-States', 'fnlwgt': '307423', 'age': '59', 'relationship': 'Not-in-family', 'race': 'Black', 'label': '0', 'education-num': '5', 'workclass': 'Private', 'capital-loss': '0', 'capital-gain': '0', 'marital-status': 'Never-married', 'sex': 'Male', 'hours-per-week': '50', 'education': '9th', 'occupation': 'Other-service'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import csv\n",
    "import sklearn\n",
    "import sklearn.feature_extraction\n",
    "\n",
    "spam = scipy.io.loadmat(\"./dist/spam_data.mat\")\n",
    "census_train = csv.DictReader(open(\"./census_training_data.csv\"))\n",
    "census_test = csv.DictReader(open(\"./census_test_data.csv\"))\n",
    "titanic_train = csv.DictReader(open(\"./titanic_training_data.csv\"))\n",
    "titanic_test = csv.DictReader(open(\"./titanic_testing_data.csv\"))\n",
    "\n",
    "firstTest = None\n",
    "for row in census_train:\n",
    "    print(row)\n",
    "    firstTest = row\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sample sequence X is empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b640deff0ba7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcensus_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mc:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\dict_vectorizer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mFeature\u001b[0m \u001b[0mvectors\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0malways\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \"\"\"\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfitting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\dict_vectorizer.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, fitting)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sample sequence X is empty.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrombuffer_empty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample sequence X is empty."
     ]
    }
   ],
   "source": [
    "\n",
    "vec = sklearn.feature_extraction.DictVectorizer()\n",
    "X = vec.fit_transform(census_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.split_rule = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.label = None \n",
    "\n",
    "def nodeDecision(node, data):\n",
    "    if node.split_rule != None:\n",
    "        if data[node.split_rule[0]] == node.split_rule[1]:\n",
    "            return nodeDecision(node.right, data)\n",
    "        else:\n",
    "            return nodeDecision(node.left, data)\n",
    "    else:\n",
    "        return node.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Where left and right are maps from labels -> frequencies\n",
    "def impurity(left, right):\n",
    "    leftEn = 0\n",
    "    rightEn = 0\n",
    "    leftSum = 0\n",
    "    rightSum = 0\n",
    "    for k,v in left.items():\n",
    "        p_c = v / len(left)\n",
    "        if v == 0: continue\n",
    "        leftEn += p_c * np.log2(p_c)\n",
    "        leftSum += v\n",
    "    for k,v in right.items():\n",
    "        p_c = v / len(right)\n",
    "        if v == 0: continue\n",
    "        rightEn += p_c * np.log2(p_c)\n",
    "        rightSum += v\n",
    "    weightSum = leftSum * leftEn + rightSum * rightEn\n",
    "    return weightSum / (leftEn + rightEn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return None if the data is essentially pure across \n",
    "def segmenter(data, labels):\n",
    "    uniqueLabels = set(labels)\n",
    "    minBadness = -1\n",
    "    maxLabelIndex = -1\n",
    "    maxLabelSplitDir = -1\n",
    "    \n",
    "    #TODO: Confirm type of data and figure out type of labels\n",
    "    \n",
    "    if len(labels) == 0:\n",
    "        return None\n",
    "    \n",
    "    firstLabel = labels[0]\n",
    "    pure = True\n",
    "    for label in labels:\n",
    "        if firstLabel != label:\n",
    "            pure = False\n",
    "            break\n",
    "    if pure:\n",
    "        return None\n",
    "    \n",
    "    for featureIndex in range(len(data[0])):\n",
    "        splits = [0] #With binary variables, a 0 split \n",
    "                        #is equivalent to a 1 split \n",
    "        for split in splits:\n",
    "            left = dict()\n",
    "            right = dict()\n",
    "            for uniqueLabel in uniqueLabels:\n",
    "                left[uniqueLabel] = 0\n",
    "                right[uniqueLabel] = 0\n",
    "            \n",
    "            \"\"\"\n",
    "            for dataIndex in range(len(data)):\n",
    "                if data[dataIndex][labelIndex] != split:\n",
    "                    for labelIndex2 in range(len(labels)):\n",
    "                        left[labelIndex2] += 1\n",
    "                else:\n",
    "                    for labelIndex2 in range(len(labels)):\n",
    "                        right[labelIndex2] += 1\n",
    "            \"\"\"\n",
    "            \n",
    "            for dataIndex in range(len(data)):\n",
    "                if data[dataIndex][featureIndex] != split:\n",
    "                    left[labels[dataIndex]] += 1\n",
    "                else:\n",
    "                    right[labels[dataIndex]] += 1\n",
    "                    \n",
    "            badness = impurity(left, right)\n",
    "            \n",
    "            #print(str(labelIndex) + \" \" + str(split) + \" \" + str(badness))\n",
    "            #print(str(left[\"a\"]) + \" \" + str(left[\"b\"]))\n",
    "            #print(str(right[\"a\"]) + \" \" + str(right[\"b\"]))\n",
    "            #print(\"----------\")\n",
    "            \n",
    "            if badness < minBadness or maxLabelIndex == -1:\n",
    "                minBadness = badness\n",
    "                maxLabelIndex = featureIndex\n",
    "                maxLabelSplitDir = split\n",
    "    return (maxLabelIndex, maxLabelSplitDir)\n",
    "\n",
    "test = np.array([\n",
    "        [1,1,0,1],\n",
    "        [1,1,0,0],\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,0,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,0,1],\n",
    "        [1,1,0,0],\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,0,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1]\n",
    "    ])\n",
    "segmenter(test, [\"a\",\"a\",\"b\",\"a\",\"b\",\"a\",\"b\",\"b\",\"b\",\"a\",\"a\",\"b\",\"a\",\"a\"])\n",
    "\n",
    "test = np.array([\n",
    "        [1,1,0,1],\n",
    "        [1,0,1,1],\n",
    "        [0,1,1,1],\n",
    "        [1,1,1,0],\n",
    "        [1,1,0,1],\n",
    "        [1,1,0,1],\n",
    "        [1,1,0,1],\n",
    "        [1,1,0,1]\n",
    "    ])\n",
    "segmenter(test, [\"a\",\"a\",\"a\",\"a\",\"b\",\"b\",\"b\",\"b\"])\n",
    "\n",
    "test = np.array([\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,0,1],\n",
    "        [1,1,0,1],\n",
    "        [1,1,0,1],\n",
    "        [1,1,0,1]\n",
    "    ])\n",
    "segmenter(test, [\"a\",\"a\",\"a\",\"a\",\"b\",\"b\",\"b\",\"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "For data set: \n",
      "[[1 1 0 1]\n",
      " [1 1 0 0]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 0 1]\n",
      " [1 1 1 1]\n",
      " [1 1 0 1]\n",
      " [1 1 0 0]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 0 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]]\n",
      "and labels: \n",
      "['a' 'a' 'b' 'a' 'b' 'a' 'b' 'b' 'b' 'a' 'a' 'b' 'a' 'a']\n",
      "A decision rule was found: \n",
      "(2, 0)\n",
      "8 6\n",
      "---------------\n",
      "For data set: \n",
      "[[1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]]\n",
      "and labels: \n",
      "['b', 'a', 'a', 'b', 'a', 'b', 'a', 'a']\n",
      "A decision rule was found: \n",
      "(0, 0)\n",
      "8 0\n",
      "---------------\n",
      "For data set: \n",
      "[[1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]]\n",
      "and labels: \n",
      "['b', 'a', 'a', 'b', 'a', 'b', 'a', 'a']\n",
      "A decision rule was found: \n",
      "(0, 0)\n",
      "8 0\n",
      "---------------\n",
      "For data set: \n",
      "[]\n",
      "and labels: \n",
      "[]\n",
      "No decision was found for a pure node\n",
      "---------------\n",
      "For data set: \n",
      "[[1 1 0 1]\n",
      " [1 1 0 0]\n",
      " [1 1 0 1]\n",
      " [1 1 0 1]\n",
      " [1 1 0 0]\n",
      " [1 1 0 1]]\n",
      "and labels: \n",
      "['a', 'a', 'b', 'b', 'b', 'a']\n",
      "A decision rule was found: \n",
      "(3, 0)\n",
      "4 2\n",
      "---------------\n",
      "For data set: \n",
      "[[1 1 0 1]\n",
      " [1 1 0 1]\n",
      " [1 1 0 1]\n",
      " [1 1 0 1]]\n",
      "and labels: \n",
      "['a', 'b', 'b', 'a']\n",
      "A decision rule was found: \n",
      "(0, 0)\n",
      "4 0\n",
      "---------------\n",
      "For data set: \n",
      "[[1 1 0 0]\n",
      " [1 1 0 0]]\n",
      "and labels: \n",
      "['a', 'b']\n",
      "A decision rule was found: \n",
      "(0, 0)\n",
      "2 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "initialNode = Node()\n",
    "\n",
    "def train(node, data, labels, levels=3):\n",
    "    if levels <= 0: return\n",
    "    decision = segmenter(data, labels)\n",
    "    print(\"---------------\")\n",
    "    print(\"For data set: \")\n",
    "    print(data)\n",
    "    print(\"and labels: \")\n",
    "    print(labels)\n",
    "    if decision != None: #A split rule was found for a non-pure node\n",
    "        print(\"A decision rule was found: \")\n",
    "        print(decision)\n",
    "        node.split_rule = decision\n",
    "        node.left = Node()\n",
    "        node.right = Node()\n",
    "        leftData = []\n",
    "        rightData = []\n",
    "        leftLabels = []\n",
    "        rightLabels = []\n",
    "        for dataIndex in range(len(data)):\n",
    "            if data[dataIndex][decision[0]] == decision[1]:\n",
    "                rightData.append(data[dataIndex])\n",
    "                rightLabels.append(labels[dataIndex])\n",
    "            else:\n",
    "                leftData.append(data[dataIndex])\n",
    "                leftLabels.append(labels[dataIndex])\n",
    "        print(str(len(leftData)) + \" \" + str(len(rightData)))\n",
    "        leftData = np.array(leftData)\n",
    "        rightData = np.array(rightData)\n",
    "        train(node.left, leftData, leftLabels, levels - 1)\n",
    "        train(node.right, rightData, rightLabels, levels - 1)\n",
    "    else:\n",
    "        print(\"No decision was found for a pure node\")\n",
    "        return\n",
    "    \n",
    "testData = np.array([\n",
    "        [1,1,0,1],\n",
    "        [1,1,0,0],\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,0,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,0,1],\n",
    "        [1,1,0,0],\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,0,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1]\n",
    "    ])\n",
    "testLabels = np.array(\n",
    "    [\"a\",\"a\",\"b\",\"a\",\"b\",\"a\",\"b\",\"b\",\"b\",\"a\",\"a\",\"b\",\"a\",\"a\"]\n",
    ")\n",
    "\n",
    "train(initialNode, testData, testLabels, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
