{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import csv\n",
    "import sklearn\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "spam = scipy.io.loadmat(\"./dist/spam_data.mat\")\n",
    "census_train = csv.DictReader(open(\"./census_training_data.csv\"))\n",
    "census_test = csv.DictReader(open(\"./census_test_data.csv\"))\n",
    "titanic_train = csv.DictReader(open(\"./titanic_training_data.csv\"))\n",
    "titanic_test = csv.DictReader(open(\"./titanic_testing_data.csv\"))\n",
    "\n",
    "def dictReaderToDictList(obj):\n",
    "    arrayDicts = []\n",
    "    for row in obj:\n",
    "        arrayDicts.append(row)\n",
    "    return arrayDicts\n",
    "\n",
    "def findMostCommonValue(dictVectorized):\n",
    "    listFeatureModes = []\n",
    "    for featureIndex in range(len(dictVectorized[0])):\n",
    "        temp = dict()\n",
    "        for dataIndex in range(len(dictVectorized)):\n",
    "            featureValue = dictVectorized[dataIndex][featureIndex]\n",
    "            if featureValue not in temp:\n",
    "                temp[featureValue] = 0\n",
    "            temp[featureValue] += 1\n",
    "        maxFeatureValue = -1\n",
    "        maxFeatureKey = -1\n",
    "        for k,v in temp.items():\n",
    "            if v > maxFeatureValue or maxFeatureKey == -1:\n",
    "                maxFeatureValue = v\n",
    "                maxFeatureKey = k\n",
    "        listFeatureModes.append(maxFeatureKey)\n",
    "    return listFeatureModes\n",
    "\n",
    "logging = False\n",
    "\n",
    "def logprint(message):\n",
    "    if logging:\n",
    "        print(message)\n",
    "    \n",
    "test = np.array([[0, 0, 1], [0, 0, 1], [2, 1, 0]])\n",
    "print(len(test[0]))\n",
    "print(findMostCommonValue(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relationship': 'Husband', 'hours-per-week': '44', 'fnlwgt': '155489', 'age': '46', 'native-country': 'United-States', 'education-num': '12', 'capital-loss': '0', 'sex': 'Male', 'capital-gain': '0', 'occupation': 'Craft-repair', 'label': '1', 'race': 'White', 'education': 'Assoc-acdm', 'marital-status': 'Married-civ-spouse', 'workclass': 'Private'}\n"
     ]
    }
   ],
   "source": [
    "spam = scipy.io.loadmat(\"./dist/spam_data.mat\")\n",
    "census_train = csv.DictReader(open(\"./census_training_data.csv\"))\n",
    "census_test = csv.DictReader(open(\"./census_test_data.csv\"))\n",
    "titanic_train = csv.DictReader(open(\"./titanic_training_data.csv\"))\n",
    "titanic_test = csv.DictReader(open(\"./titanic_testing_data.csv\"))\n",
    "\n",
    "arrayDictCensusTrain = dictReaderToDictList(census_train)\n",
    "print(arrayDictCensusTrain[567])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
      "        sparse=False)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "22267\n",
      "[[ 0.  0.  0. ...,  0.  0.  1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ncensus_train = csv.DictReader(open(\"./census_training_data.csv\"))\\nprocessed_census_train = []\\ni = 0\\nfor row in census_train:\\n    if i >= 3: break\\n    i += 1\\n    processed_census_train.append(vec.transform(row))\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam = scipy.io.loadmat(\"./dist/spam_data.mat\")\n",
    "census_train = csv.DictReader(open(\"./census_training_data.csv\"))\n",
    "census_test = csv.DictReader(open(\"./census_test_data.csv\"))\n",
    "titanic_train = csv.DictReader(open(\"./titanic_training_data.csv\"))\n",
    "titanic_test = csv.DictReader(open(\"./titanic_testing_data.csv\"))\n",
    "\n",
    "census_vec = sklearn.feature_extraction.DictVectorizer(sparse=False)\n",
    "census_vec.fit(census_train)\n",
    "\n",
    "print(census_vec)\n",
    "\n",
    "randTest = {'label': '0', 'hours-per-week': '50', 'education-num': '5', 'occupation': 'Other-service', 'marital-status': 'Never-married', 'relationship': 'Not-in-family', 'native-country': 'United-States', 'sex': 'Male', 'age': '59', 'workclass': 'Private', 'fnlwgt': '307423', 'race': 'Black', 'education': '9th', 'capital-loss': '0', 'capital-gain': '0'}\n",
    "otherTest = {'label': '0', 'hours-per-week': '45', 'education-num': '12', 'occupation': 'Sales', 'marital-status': 'Married-civ-spouse', 'relationship': 'Husband', 'native-country': 'United-States', 'sex': 'Male', 'age': '51', 'workclass': 'Without-pay', 'fnlwgt': '124963', 'race': 'White', 'education': 'Assoc-acdm', 'capital-loss': '0', 'capital-gain': '0'}\n",
    "\n",
    "temp = census_vec.transform(randTest)\n",
    "\n",
    "print(temp)\n",
    "print(len(temp[0]))\n",
    "print(census_vec.transform(otherTest))\n",
    "\n",
    "\"\"\"\n",
    "census_train = csv.DictReader(open(\"./census_training_data.csv\"))\n",
    "processed_census_train = []\n",
    "i = 0\n",
    "for row in census_train:\n",
    "    if i >= 3: break\n",
    "    i += 1\n",
    "    processed_census_train.append(vec.transform(row))\n",
    "\"\"\"\n",
    "\n",
    "#np.savetxt(\"preprocessed_census_train.csv\", processed_census_train, fmt=\"%i,%i\", delimiter=\",\", header=\"Id,Category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relationship': 'Husband', 'hours-per-week': 44, 'fnlwgt': 155489, 'age': 46, 'native-country': 'United-States', 'education-num': 12, 'capital-loss': 0, 'sex': 'Male', 'capital-gain': 0, 'occupation': 'Craft-repair', 'race': 'White', 'education': 'Assoc-acdm', 'marital-status': 'Married-civ-spouse', 'workclass': 'Private'}\n",
      "{'relationship': 'Not-in-family', 'hours-per-week': 40, 'fnlwgt': 341757, 'age': 42, 'native-country': 'United-States', 'education-num': 10, 'capital-loss': 0, 'sex': 'Female', 'capital-gain': 0, 'occupation': 'Machine-op-inspct', 'race': 'White', 'education': 'Some-college', 'marital-status': 'Never-married', 'workclass': 'Private'}\n",
      "32724\n"
     ]
    }
   ],
   "source": [
    "f=pd.read_csv(\"./census_training_data.csv\")\n",
    "keep_col = [\n",
    "            'age',\n",
    "            'workclass',\n",
    "            'fnlwgt',\n",
    "            'education',\n",
    "            'education-num',\n",
    "            'marital-status',\n",
    "            'occupation',\n",
    "            'relationship',\n",
    "            'race',\n",
    "            'sex',\n",
    "            'capital-gain',\n",
    "            'capital-loss',\n",
    "            'hours-per-week',\n",
    "            'native-country'\n",
    "           ]\n",
    "new_f = f[keep_col]\n",
    "\n",
    "census_train_labels = f['label'].tolist()\n",
    "\n",
    "def collateDataFrame(frame):\n",
    "    rows = -1\n",
    "    results = []\n",
    "    for columnName, dictColumnData in frame.items():\n",
    "        if rows == -1:\n",
    "            rows = len(dictColumnData)\n",
    "            results = [{} for _ in range(rows)]\n",
    "        for index, value in dictColumnData.items():\n",
    "            results[index][columnName] = value\n",
    "    return results      \n",
    "        \n",
    "census_train_csv = collateDataFrame(new_f)\n",
    "\n",
    "print(census_train_csv[567])\n",
    "print(census_train_csv[25674])\n",
    "print(len(census_train_csv))\n",
    "\n",
    "#new_f.to_csv(\"newFile.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "census_vec = sklearn.feature_extraction.DictVectorizer(sparse=False)\n",
    "census_vec.fit(census_train_csv)\n",
    "\n",
    "logprint(census_vec)\n",
    "\n",
    "randTest = {'marital-status': 'Married-civ-spouse', 'race': 'White', 'hours-per-week': 44, 'education': 'Assoc-acdm', 'education-num': 12, 'age': 46, 'capital-gain': 0, 'occupation': 'Craft-repair', 'fnlwgt': 155489, 'workclass': 'Private', 'relationship': 'Husband', 'sex': 'Male', 'capital-loss': 0, 'native-country': 'United-States'}\n",
    "otherTest = {'marital-status': 'Married-civ-spouse', 'race': 'White', 'hours-per-week': 40, 'education': 'Some-college', 'education-num': 10, 'age': 34, 'capital-gain': 0, 'occupation': 'Handlers-cleaners', 'fnlwgt': 207668, 'workclass': 'Private', 'relationship': 'Husband', 'sex': 'Male', 'capital-loss': 1887, 'native-country': 'United-States'}\n",
    "\n",
    "temp = census_vec.transform(randTest)\n",
    "\n",
    "logprint(temp)\n",
    "logprint(len(temp[0]))\n",
    "logprint(census_vec.transform(otherTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"--------------\")\\n\\nvec = sklearn.feature_extraction.DictVectorizer(sparse=False)\\ntestCensusRandom = {\\'label\\': \\'0\\', \\'hours-per-week\\': \\'50\\', \\'education-num\\': \\'5\\', \\'occupation\\': \\'Other-service\\', \\'marital-status\\': \\'Never-married\\', \\'relationship\\': \\'Not-in-family\\', \\'native-country\\': \\'United-States\\', \\'sex\\': \\'Male\\', \\'age\\': \\'59\\', \\'workclass\\': \\'Private\\', \\'fnlwgt\\': \\'307423\\', \\'race\\': \\'Black\\', \\'education\\': \\'9th\\', \\'capital-loss\\': \\'0\\', \\'capital-gain\\': \\'0\\'}\\nvec.fit([testCensusRandom])\\n\\notherCensusRandom = {\\'label\\': \\'0\\', \\'hours-per-week\\': \\'45\\', \\'education-num\\': \\'12\\', \\'occupation\\': \\'Sales\\', \\'marital-status\\': \\'Married-civ-spouse\\', \\'relationship\\': \\'Husband\\', \\'native-country\\': \\'United-States\\', \\'sex\\': \\'Male\\', \\'age\\': \\'51\\', \\'workclass\\': \\'Without-pay\\', \\'fnlwgt\\': \\'124963\\', \\'race\\': \\'White\\', \\'education\\': \\'Assoc-acdm\\', \\'capital-loss\\': \\'0\\', \\'capital-gain\\': \\'0\\'}\\nprint(vec.transform(testCensusRandom))\\nprint(vec.transform(otherCensusRandom))\\n'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#vec = sklearn.feature_extraction.DictVectorizer()\n",
    "#X = vec.fit_transform(census_train)\n",
    "\n",
    "#enc = sklearn.preprocessing.OneHotEncoder()\n",
    "#result = enc.fit(arrayDicts)\n",
    "\n",
    "#print([{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}])\n",
    "#print(dictReaderToDictList(census_train))\n",
    "\n",
    "\"\"\"\n",
    "### Abandon all hope ye who enter here ###\n",
    "\n",
    "census_train = csv.DictReader(open(\"./census_training_data.csv\"))\n",
    "\n",
    "firstData = []\n",
    "i = 0\n",
    "for row in census_train:\n",
    "    firstData.append(row)\n",
    "    i += 1\n",
    "    if i > 1000:\n",
    "        break   \n",
    "    \n",
    "vec = sklearn.feature_extraction.DictVectorizer(sparse=False)\n",
    "vec.fit(firstData)\n",
    "\n",
    "census_vec_not_binary = []\n",
    "\n",
    "census_train = csv.DictReader(open(\"./census_training_data.csv\"))\n",
    "\n",
    "for row in census_train:\n",
    "    census_vec_not_binary.append(vec.transform(row))\n",
    "    \n",
    "census_vec_not_binary = np.array(census_vec_not_binary)\n",
    "\n",
    "print(census_vec_not_binary[0])\n",
    "\n",
    "census_train = csv.DictReader(open(\"./census_training_data.csv\"))\n",
    "\n",
    "i = 0\n",
    "for row in census_train: \n",
    "    print(row)\n",
    "    print(census_vec_not_binary[i])\n",
    "    i += 1\n",
    "    if i >= 50: break\n",
    "\"\"\"        \n",
    "        \n",
    "#inv_result = vec.inverse_transform(result)\n",
    "\n",
    "#enc = sklearn.preprocessing.OneHotEncoder()\n",
    "#binaryResult = enc.fit_transform(result)\n",
    "\n",
    "#print(result)\n",
    "#print(inv_result)\n",
    "\n",
    "\"\"\"\n",
    "print(\"--------------\")\n",
    "\n",
    "vec = sklearn.feature_extraction.DictVectorizer(sparse=False)\n",
    "testCensusRandom = {'label': '0', 'hours-per-week': '50', 'education-num': '5', 'occupation': 'Other-service', 'marital-status': 'Never-married', 'relationship': 'Not-in-family', 'native-country': 'United-States', 'sex': 'Male', 'age': '59', 'workclass': 'Private', 'fnlwgt': '307423', 'race': 'Black', 'education': '9th', 'capital-loss': '0', 'capital-gain': '0'}\n",
    "vec.fit([testCensusRandom])\n",
    "\n",
    "otherCensusRandom = {'label': '0', 'hours-per-week': '45', 'education-num': '12', 'occupation': 'Sales', 'marital-status': 'Married-civ-spouse', 'relationship': 'Husband', 'native-country': 'United-States', 'sex': 'Male', 'age': '51', 'workclass': 'Without-pay', 'fnlwgt': '124963', 'race': 'White', 'education': 'Assoc-acdm', 'capital-loss': '0', 'capital-gain': '0'}\n",
    "print(vec.transform(testCensusRandom))\n",
    "print(vec.transform(otherCensusRandom))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.split_rule = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.label = None\n",
    "\n",
    "def nodeDecision(node, data):\n",
    "    if node.split_rule != None:\n",
    "        logprint(\"Feature \" + str(node.split_rule[0]) + \", split across: \" + str(node.split_rule[1]) + \n",
    "                 \", found \" + str(data[node.split_rule[0]]))\n",
    "        if data[node.split_rule[0]] > node.split_rule[1]:\n",
    "            logprint(\"Moving to right...\")\n",
    "            return nodeDecision(node.right, data)\n",
    "        else:\n",
    "            logprint(\"Moving to left...\")\n",
    "            return nodeDecision(node.left, data)\n",
    "    else:\n",
    "        logprint(\"Ended at the decision: \")\n",
    "        logprint(str(node.label))\n",
    "        return node.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Where left and right are maps from labels -> frequencies\n",
    "def impurity(left, right):\n",
    "    leftEn = 0\n",
    "    rightEn = 0\n",
    "    leftSum = 0\n",
    "    rightSum = 0\n",
    "    for k,v in left.items():\n",
    "        p_c = v / len(left)\n",
    "        if v == 0: continue\n",
    "        leftEn += p_c * np.log2(p_c)\n",
    "        leftSum += v\n",
    "    for k,v in right.items():\n",
    "        p_c = v / len(right)\n",
    "        if v == 0: continue\n",
    "        rightEn += p_c * np.log2(p_c)\n",
    "        rightSum += v\n",
    "    weightSum = leftSum * leftEn + rightSum * rightEn\n",
    "    return weightSum / (leftEn + rightEn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 0.5)\n",
      "(2, 0.5)\n",
      "guess\n"
     ]
    }
   ],
   "source": [
    "# Return None if the data is essentially pure across \n",
    "def segmenter(data, labels, skipFeatures=[]):\n",
    "    uniqueLabels = set(labels)\n",
    "    minBadness = -1\n",
    "    maxLabelIndex = -1\n",
    "    maxLabelSplitDir = -1\n",
    "    \n",
    "    #TODO: Confirm type of data and figure out type of labels\n",
    "    \n",
    "    if len(labels) == 0:\n",
    "        return None\n",
    "    \n",
    "    firstLabel = labels[0]\n",
    "    pure = True\n",
    "    for label in labels:\n",
    "        if firstLabel != label:\n",
    "            pure = False\n",
    "            break\n",
    "    if pure:\n",
    "        return None\n",
    "    \n",
    "    featuresEqual = True\n",
    "    for featureIndex in range(len(data[0])):\n",
    "        if featureIndex in skipFeatures: continue\n",
    "        equalNum = data[0][featureIndex]\n",
    "        equal = True\n",
    "        for dataIndex in range(len(data)):\n",
    "            equal = equal and (equalNum == data[dataIndex][featureIndex]) \n",
    "        if not equal: \n",
    "            featuresEqual = False\n",
    "            break\n",
    "        featuresEqual = featuresEqual and equal\n",
    "    if featuresEqual:\n",
    "        return \"guess\"\n",
    "    \n",
    "    for featureIndex in range(len(data[0])):\n",
    "        #splits = [] #With binary variables, a 0 split \n",
    "                        #is equivalent to a 1 split \n",
    "        minSplit = None\n",
    "        maxSplit = None\n",
    "        for dataIndex in range(len(data)):\n",
    "            if maxSplit == None or data[dataIndex][featureIndex] > maxSplit:\n",
    "                maxSplit = data[dataIndex][featureIndex]\n",
    "            if minSplit == None or data[dataIndex][featureIndex] < minSplit:\n",
    "                minSplit = data[dataIndex][featureIndex]\n",
    "        splits = [(minSplit + maxSplit) / 2.0]       \n",
    "\n",
    "        for split in splits:\n",
    "            left = dict()\n",
    "            right = dict()\n",
    "            for uniqueLabel in uniqueLabels:\n",
    "                left[uniqueLabel] = 0\n",
    "                right[uniqueLabel] = 0\n",
    "            \n",
    "            \"\"\"\n",
    "            for dataIndex in range(len(data)):\n",
    "                if data[dataIndex][labelIndex] != split:\n",
    "                    for labelIndex2 in range(len(labels)):\n",
    "                        left[labelIndex2] += 1\n",
    "                else:\n",
    "                    for labelIndex2 in range(len(labels)):\n",
    "                        right[labelIndex2] += 1\n",
    "            \"\"\"\n",
    "            \n",
    "            for dataIndex in range(len(data)):\n",
    "                if data[dataIndex][featureIndex] <= split:\n",
    "                    left[labels[dataIndex]] += 1\n",
    "                else:\n",
    "                    right[labels[dataIndex]] += 1\n",
    "                    \n",
    "            badness = impurity(left, right)\n",
    "            \n",
    "            #print(str(featureIndex) + \" \" + str(split) + \" \" + str(badness))\n",
    "            #print(str(left[\"a\"]) + \" \" + str(left[\"b\"]))\n",
    "            #print(str(right[\"a\"]) + \" \" + str(right[\"b\"]))\n",
    "            #print(badness)\n",
    "            #print(\"----------\")\n",
    "            \n",
    "            if badness < minBadness or maxLabelIndex == -1:\n",
    "                minBadness = badness\n",
    "                maxLabelIndex = featureIndex\n",
    "                maxLabelSplitDir = split\n",
    "    \n",
    "    return (maxLabelIndex, maxLabelSplitDir)\n",
    "\n",
    "test = np.array([\n",
    "        [1,1,0,1],\n",
    "        [1,1,0,0],\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,0,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,0,1],\n",
    "        [1,1,0,0],\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,0,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1]\n",
    "    ])\n",
    "print(segmenter(test, [\"a\",\"a\",\"b\",\"a\",\"b\",\"a\",\"b\",\"b\",\"b\",\"a\",\"a\",\"b\",\"a\",\"a\"]))\n",
    "\n",
    "test = np.array([\n",
    "        [1,1,0,1],\n",
    "        [1,0,1,1],\n",
    "        [0,1,1,1],\n",
    "        [1,1,1,0],\n",
    "        [1,1,0,1],\n",
    "        [1,1,0,1],\n",
    "        [1,1,0,1],\n",
    "        [1,1,0,1]\n",
    "    ])\n",
    "print(segmenter(test, [\"a\",\"a\",\"a\",\"a\",\"b\",\"b\",\"b\",\"b\"]))\n",
    "\n",
    "test = np.array([\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1],\n",
    "    ])\n",
    "print(segmenter(test, [\"a\",\"a\",\"a\",\"b\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "#initialNode = Node()\n",
    "\n",
    "def train(node, data, labels, levels=3, randomForest=False):\n",
    "    logprint(\"---------------\")\n",
    "    \n",
    "    if len(labels) <= 0: return\n",
    "    \n",
    "    if levels <= 0:\n",
    "        uniqueLabels = set(labels)\n",
    "        logprint(\"A guessing rule was found for data at the lowest depth: \")\n",
    "        labelsByCount = dict()\n",
    "        for label in uniqueLabels:\n",
    "            labelsByCount[label] = 0\n",
    "        for label in labels:\n",
    "            labelsByCount[label] += 1\n",
    "        maxLabel = None\n",
    "        maxLabelScore = -1;\n",
    "        for label in uniqueLabels:\n",
    "            if labelsByCount[label] > maxLabelScore or maxLabel == None:\n",
    "                maxScore = labelsByCount[label]\n",
    "                maxLabel = label\n",
    "        node.label = maxLabel\n",
    "        return\n",
    "        \n",
    "    decision = None\n",
    "    if randomForest:\n",
    "        numFeatures = len(data[0])\n",
    "        #desiredLenSampleFeatures = int(math.ceil(math.sqrt(numFeatures)))\n",
    "        desiredLenSampleFeatures = int(numFeatures / 3.0)\n",
    "        sampleFeatures = []\n",
    "        while True: #randomly pick m of d features specified in note 16\n",
    "            randFeature = int(random.random() * numFeatures)\n",
    "            if randFeature not in sampleFeatures:\n",
    "                sampleFeatures.append(randFeature)\n",
    "            if len(sampleFeatures) >= desiredLenSampleFeatures:\n",
    "                break\n",
    "\n",
    "        skipFeatures = [i for i in range(numFeatures) if i not in sampleFeatures]    \n",
    "\n",
    "        decision = segmenter(data, labels, skipFeatures)\n",
    "    else:\n",
    "        decision = segmenter(data, labels)\n",
    "\n",
    "    \n",
    "    logprint(\"For data set: ...\")\n",
    "    #print(data)\n",
    "    logprint(\"and labels: \")\n",
    "    logprint(labels)\n",
    "\n",
    "    if decision != None: #A split rule was found for a non-pure node\n",
    "        if decision == \"guess\": #It's a decision for non-separable data: \n",
    "            uniqueLabels = set(labels)\n",
    "            logprint(\"A guessing rule was found for non-separable data: \")\n",
    "            labelsByCount = dict()\n",
    "            for label in uniqueLabels:\n",
    "                labelsByCount[label] = 0\n",
    "            for label in labels:\n",
    "                labelsByCount[label] += 1\n",
    "            maxLabel = None\n",
    "            maxLabelScore = -1;\n",
    "            for label in uniqueLabels:\n",
    "                if labelsByCount[label] > maxLabelScore or maxLabel == None:\n",
    "                    maxScore = labelsByCount[label]\n",
    "                    maxLabel = label\n",
    "            node.label = maxLabel\n",
    "        else:\n",
    "            logprint(\"A decision rule was found: \")\n",
    "            logprint(decision)\n",
    "            node.split_rule = decision\n",
    "            node.left = Node()\n",
    "            node.right = Node()\n",
    "            leftData = []\n",
    "            rightData = []\n",
    "            leftLabels = []\n",
    "            rightLabels = []\n",
    "            for dataIndex in range(len(data)):\n",
    "                if data[dataIndex][decision[0]] > decision[1]:\n",
    "                    rightData.append(data[dataIndex])\n",
    "                    rightLabels.append(labels[dataIndex])\n",
    "                else:\n",
    "                    leftData.append(data[dataIndex])\n",
    "                    leftLabels.append(labels[dataIndex])\n",
    "            logprint(str(len(leftData)) + \" \" + str(len(rightData)))\n",
    "            leftData = np.array(leftData)\n",
    "            rightData = np.array(rightData)\n",
    "            train(node.left, leftData, leftLabels, levels - 1)\n",
    "            train(node.right, rightData, rightLabels, levels - 1)\n",
    "    else:\n",
    "        logprint(\"No decision was found for a pure node\")\n",
    "        #if len(labels) == 0: return\n",
    "        node.label = labels[0]\n",
    "    \n",
    "def fullTrain(data, labels, levels=3, skipFeatures=[]):\n",
    "    initialNode = Node()\n",
    "    train(initialNode, data, labels, levels, skipFeatures)\n",
    "    return initialNode\n",
    "    \n",
    "testData = np.array([\n",
    "        [1,1,0,1],\n",
    "        [1,1,0,0],\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,0,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,0,1],\n",
    "        [1,1,0,0],\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,0,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1],\n",
    "        [1,1,1,1],\n",
    "        [0,0,0,0]\n",
    "    ])\n",
    "testLabels = np.array(\n",
    "    [\"a\",\"a\",\"b\",\"a\",\"b\",\"a\",\"b\",\"b\",\"b\",\"a\",\"a\",\"b\",\"a\",\"a\",\"c\"]\n",
    ")\n",
    "\n",
    "initialNode = fullTrain(testData, testLabels, 5)\n",
    "\n",
    "print(nodeDecision(initialNode, [0,0,0,0]))\n",
    "print(nodeDecision(initialNode, [1,1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def randomForestTrain(data, labels, levels = 25, trees = 10):\n",
    "    numFeatu\n",
    "    randomForestRoots = []\n",
    "    for indexTree in range(trees):\n",
    "        print(indexTree)\n",
    "        \n",
    "        copyData = np.array(data)\n",
    "        #for featureIndex in range(numFeatures):\n",
    "            #if featureIndex not in sampleFeatures:\n",
    "                #for dataIndex in range(len(data)):\n",
    "                    #copyData[dataIndex][featureIndex] = 0\n",
    "        \n",
    "        indices = [i for i in range(len(data))]\n",
    "        chosenIndices = []\n",
    "        for i in range(int(len(data) * 0.2)):\n",
    "            randIndex = int(random.random() * len(indices)) \n",
    "            chosenIndices.append(indices.pop(randIndex))\n",
    "        \n",
    "        copyDataSubset = []\n",
    "        copyLabels = []\n",
    "        for chosen in chosenIndices:\n",
    "            copyDataSubset.append(copyData[chosen])\n",
    "            copyLabels.append(labels[chosen])\n",
    "        copyDataSubset = np.array(copyDataSubset)\n",
    "        \n",
    "        \"\"\"\n",
    "        print(copyDataSubset)\n",
    "        print(copyLabels)\n",
    "        print(copyDataSubset[0])\n",
    "        print(\"------\")\n",
    "        \"\"\"\n",
    "        \n",
    "        treeRoot = fullTrain(copyDataSubset, copyLabels, levels)\n",
    "        \n",
    "        randomForestRoots.append(treeRoot)\n",
    "    return randomForestRoots\n",
    "\n",
    "def randomForestDecide(randomForestRoots, decisionPoint):\n",
    "    candidates = dict()\n",
    "    for forestRoot in randomForestRoots:\n",
    "        prediction = nodeDecision(forestRoot, decisionPoint)\n",
    "        if prediction not in candidates:\n",
    "            candidates[prediction] = 0\n",
    "        candidates[prediction] += 1\n",
    "    maxLabel = None\n",
    "    maxLabelValue = -1\n",
    "    for k,v in candidates.items():\n",
    "        if maxLabel == None or v > maxLabelValue:\n",
    "            maxLabel = k\n",
    "            maxLabelValue = v\n",
    "    logprint(candidates)\n",
    "    logprint(maxLabel)\n",
    "    return maxLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[50, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "listy = [1,2,3]\n",
    "nparr = list(listy)\n",
    "nparr[0] = 50;\n",
    "print(listy)\n",
    "print(nparr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 24, split across: 0.5, found 0.0\n",
      "Moving to left...\n",
      "Feature 97, split across: 0.5, found 0.0\n",
      "Moving to left...\n",
      "Feature 92, split across: 0.5, found 1.0\n",
      "Moving to right...\n",
      "Feature 26, split across: 0.5, found 0.0\n",
      "Moving to left...\n",
      "Feature 15, split across: 0.5, found 1.0\n",
      "Moving to right...\n",
      "Feature 22, split across: 0.5, found 1.0\n",
      "Moving to right...\n",
      "Feature 74, split across: 0.5, found 0.0\n",
      "Moving to left...\n",
      "Feature 0, split across: 49.0, found 54.0\n",
      "Moving to right...\n",
      "Feature 103, split across: 0.5, found 0.0\n",
      "Moving to left...\n",
      "Feature 105, split across: 0.5, found 0.0\n",
      "Moving to left...\n",
      "Feature 75, split across: 0.5, found 0.0\n",
      "Moving to left...\n",
      "Feature 71, split across: 0.5, found 1.0\n",
      "Moving to right...\n",
      "Feature 1, split across: 4307.0, found 0.0\n",
      "Moving to left...\n",
      "Feature 20, split across: 147908.5, found 156877.0\n",
      "Moving to right...\n",
      "Ended at the decision: \n",
      "0\n",
      "Feature 24, split across: 0.5, found 0.0\n",
      "Moving to left...\n",
      "Feature 97, split across: 0.5, found 0.0\n",
      "Moving to left...\n",
      "Feature 92, split across: 0.5, found 1.0\n",
      "Moving to right...\n",
      "Feature 26, split across: 0.5, found 0.0\n",
      "Moving to left...\n",
      "Feature 15, split across: 0.5, found 1.0\n",
      "Moving to right...\n",
      "Feature 22, split across: 0.5, found 1.0\n",
      "Moving to right...\n",
      "Feature 74, split across: 0.5, found 0.0\n",
      "Moving to left...\n",
      "Feature 0, split across: 49.0, found 58.0\n",
      "Moving to right...\n",
      "Feature 103, split across: 0.5, found 1.0\n",
      "Moving to right...\n",
      "Feature 85, split across: 0.5, found 0.0\n",
      "Moving to left...\n",
      "Feature 79, split across: 0.5, found 0.0\n",
      "Moving to left...\n",
      "Feature 83, split across: 0.5, found 1.0\n",
      "Moving to right...\n",
      "Ended at the decision: \n",
      "0\n",
      "0.791\n"
     ]
    }
   ],
   "source": [
    "#arrayDictCensusTrain\n",
    "\n",
    "#logprint(arrayDictCensusTrain[297])\n",
    "\n",
    "trainingSetTransformed = census_vec.transform(census_train_csv[1:30000])\n",
    "validSetTransformed = census_vec.transform(census_train_csv[30000:31000])\n",
    "\n",
    "logprint(trainingSetTransformed)\n",
    "logprint(census_train_labels[30000:31000])\n",
    "\n",
    "trainedCensusNode = fullTrain(trainingSetTransformed, census_train_labels[1:30000], 25)\n",
    "validationLabels = census_train_labels[30000:31000]\n",
    "\n",
    "accuracy = 0\n",
    "\n",
    "logging = True\n",
    "\n",
    "for i in range(len(validSetTransformed)):\n",
    "    if i >= 2: logging = False\n",
    "    transformed = validSetTransformed[i]\n",
    "    prediction = nodeDecision(trainedCensusNode, transformed)\n",
    "    #print(str(prediction) + \" \" + str(validationLabels[i]))\n",
    "    if str(prediction) == str(validationLabels[i]):\n",
    "        accuracy += 1\n",
    "\n",
    "accuracy /= len(validationLabels)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'numFeatures' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-d5a4e8da7680>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mforestRoots\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandomForestTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingSetTransformed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcensus_train_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogging\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-cd5f93420bb5>\u001b[0m in \u001b[0;36mrandomForestTrain\u001b[0;34m(data, labels, levels, trees)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mcopyData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[1;32mfor\u001b[0m \u001b[0mfeatureIndex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumFeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfeatureIndex\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msampleFeatures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mdataIndex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'numFeatures' is not defined"
     ]
    }
   ],
   "source": [
    "forestRoots = randomForestTrain(trainingSetTransformed, census_train_labels, 25, 15)\n",
    "\n",
    "logging = True\n",
    "\n",
    "accuracy = 0\n",
    "\n",
    "for i in range(len(validSetTransformed)):\n",
    "    if i >= 10: logging = False\n",
    "    transformed = validSetTransformed[i]\n",
    "    #prediction = nodeDecision(trainedCensusNode, transformed)\n",
    "    prediction = randomForestDecide(forestRoots, transformed)\n",
    "    #print(str(prediction) + \" \" + str(validationLabels[i]))\n",
    "    if str(prediction) == str(validationLabels[i]):\n",
    "        accuracy += 1\n",
    "        \n",
    "accuracy /= len(validationLabels)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
